{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpBi/tXNWUquX4q/auUHKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAFS20/RIGEL/blob/main/RIGEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# RIGEL v1.1 — Robust Interpretable Generalized Ensemble Learner\n",
        "# Autor: Ricardo Alonzo Fernández Salguero\n",
        "\n",
        "\n",
        "import os, sys, math, json, time, textwrap, warnings, typing, inspect\n",
        "from typing import Tuple, Dict, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "SEED = 123\n",
        "RNG = np.random.default_rng(SEED)\n",
        "\n",
        "# --------------------------------- Utilities ----------------------------------\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "OUT_DIR = \"/content/rigel_out\"\n",
        "ensure_dir(OUT_DIR)\n",
        "\n",
        "def hdr(title: str):\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(title)\n",
        "    print(\"=\"*90)\n",
        "\n",
        "def env_report():\n",
        "    import sklearn, numpy, pandas, matplotlib\n",
        "    rep = {\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"numpy\": numpy.__version__,\n",
        "        \"pandas\": pandas.__version__,\n",
        "        \"matplotlib\": matplotlib.__version__,\n",
        "        \"sklearn\": sklearn.__version__,\n",
        "    }\n",
        "    print(\"Environment:\", rep)\n",
        "    return rep\n",
        "\n",
        "# --------------------------- Synthetic Data Generators ------------------------\n",
        "def inject_missing_and_outliers(X: np.ndarray, missing_frac=0.05, outlier_frac=0.02, outlier_scale=10.0):\n",
        "    X = X.copy()\n",
        "    n, d = X.shape\n",
        "    m_mask = RNG.random((n, d)) < missing_frac\n",
        "    X[m_mask] = np.nan\n",
        "    o_mask = RNG.random((n, d)) < outlier_frac\n",
        "    X[o_mask] = X[o_mask] * RNG.uniform(0, outlier_scale, size=o_mask.sum())\n",
        "    return X\n",
        "\n",
        "def make_mixed_features(n: int, d_num: int, d_cat: int, seed=None):\n",
        "    r = np.random.default_rng(seed)\n",
        "    X_num = r.normal(size=(n, d_num))\n",
        "    cats = []\n",
        "    for j in range(d_cat):\n",
        "        k = r.integers(2, 8)  # 2..7 categories\n",
        "        z = r.normal(size=n)\n",
        "        edges = np.quantile(z, np.linspace(0, 1, k+1)[1:-1])\n",
        "        xj = np.digitize(z, edges)\n",
        "        cats.append(xj.reshape(-1,1))\n",
        "    X_cat = np.hstack(cats) if d_cat>0 else np.empty((n,0))\n",
        "    X = np.hstack([X_num, X_cat])\n",
        "    return X, d_num, d_cat\n",
        "\n",
        "def gen_classification_dataset(n=3000, d_num=8, d_cat=4, kind=\"mixed\", seed=None, imb=0.65):\n",
        "    r = np.random.default_rng(seed)\n",
        "    X, dnum, dcat = make_mixed_features(n, d_num, d_cat, seed=seed)\n",
        "    num = X[:, :dnum] if dnum>0 else np.empty((n,0))\n",
        "    cat = X[:, dnum:] if dcat>0 else np.empty((n,0))\n",
        "\n",
        "    emb = []\n",
        "    for j in range(dcat):\n",
        "        k = int(cat[:, j].max()) + 1\n",
        "        E = r.normal(size=(k, 3))\n",
        "        emb.append(E[cat[:, j].astype(int)])\n",
        "    Ecat = np.hstack(emb) if dcat>0 else np.zeros((n,0))\n",
        "\n",
        "    if kind == \"linear\":\n",
        "        z = (num @ r.normal(size=dnum)) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"step\":\n",
        "        z = np.where((num[:,0] + (num[:,1]**2 if dnum>1 else 0)) > 0, 2.0, -2.0) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"sinusoidal\":\n",
        "        z = np.sin(num[:,0]*2.5) + (np.cos(num[:,1]*1.3) if dnum>1 else 0) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"xor\":\n",
        "        a = (num[:,0] > 0).astype(int) if dnum>0 else r.integers(0,2,size=n)\n",
        "        b = (num[:,1] > 0).astype(int) if dnum>1 else r.integers(0,2,size=n)\n",
        "        z = np.where(a ^ b, 2.0, -2.0) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    else:  # mixed\n",
        "        z = (\n",
        "            0.8*np.sin(num[:,0]) +\n",
        "            0.5*(num[:,1]**2 if dnum>1 else 0) +\n",
        "            0.3*(num[:,2]*num[:,3] if dnum>3 else 0) +\n",
        "            0.6*(Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "        )\n",
        "    noise = r.normal(scale=1 + 0.5*np.abs(num[:,0]) if dnum>0 else 1, size=n)\n",
        "    logits = z + 0.8*noise\n",
        "    thr = np.quantile(logits, imb)\n",
        "    y = (logits > thr).astype(int)\n",
        "\n",
        "    X = inject_missing_and_outliers(X, missing_frac=0.05, outlier_frac=0.02, outlier_scale=12.)\n",
        "    return X, y, dnum, dcat\n",
        "\n",
        "def gen_regression_dataset(n=3000, d_num=8, d_cat=4, kind=\"mixed\", seed=None):\n",
        "    r = np.random.default_rng(seed)\n",
        "    X, dnum, dcat = make_mixed_features(n, d_num, d_cat, seed=seed)\n",
        "    num = X[:, :dnum] if dnum>0 else np.empty((n,0))\n",
        "    cat = X[:, dnum:] if dcat>0 else np.empty((n,0))\n",
        "\n",
        "    emb = []\n",
        "    for j in range(dcat):\n",
        "        k = int(cat[:, j].max()) + 1\n",
        "        E = r.normal(size=(k, 3))\n",
        "        emb.append(E[cat[:, j].astype(int)])\n",
        "    Ecat = np.hstack(emb) if dcat>0 else np.zeros((n,0))\n",
        "\n",
        "    if kind == \"linear\":\n",
        "        y = (num @ r.normal(size=dnum)) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"step\":\n",
        "        y = np.where((num[:,0] + (num[:,1]**2 if dnum>1 else 0)) > 0, 3.0, -1.5) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"sinusoidal\":\n",
        "        y = 2*np.sin(num[:,0]*2.0) + (np.cos(num[:,1]*1.7) if dnum>1 else 0) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "    elif kind == \"hetero\":\n",
        "        base = (num[:,0] if dnum>0 else 0) + (Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "        sigma = 0.3 + 0.7*np.abs(np.sin(base))\n",
        "        y = base + r.normal(scale=sigma)\n",
        "    else:  # mixed\n",
        "        y = (\n",
        "            1.2*np.sin(num[:,0]) +\n",
        "            0.7*(num[:,1]**2 if dnum>1 else 0) +\n",
        "            0.4*(num[:,2]*num[:,3] if dnum>3 else 0) +\n",
        "            0.8*(Ecat @ r.normal(size=Ecat.shape[1]))\n",
        "        )\n",
        "    y = y + r.normal(scale=0.5, size=y.shape[0])\n",
        "    X = inject_missing_and_outliers(X, missing_frac=0.05, outlier_frac=0.02, outlier_scale=12.)\n",
        "    return X, y, dnum, dcat\n",
        "\n",
        "# ------------------------------ Models & Prep ---------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer, PowerTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, log_loss, brier_score_loss,\n",
        "    r2_score, mean_squared_error, mean_absolute_error\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Optional external baselines\n",
        "HAVE_XGB = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    HAVE_XGB = False\n",
        "\n",
        "def make_onehot_kwargs():\n",
        "    # sklearn >=1.4 usa sparse_output; versiones previas usan sparse\n",
        "    sig = inspect.signature(OneHotEncoder.__init__)\n",
        "    if 'sparse_output' in sig.parameters:\n",
        "        return dict(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    else:\n",
        "        return dict(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def make_preprocessor(dnum, dcat, for_regression=False):\n",
        "    num_idx = list(range(dnum))\n",
        "    cat_idx = list(range(dnum, dnum+dcat))\n",
        "    transformers = []\n",
        "    if dnum>0:\n",
        "        steps = [(\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                 (\"qt\", QuantileTransformer(output_distribution=\"normal\", subsample=10000))]\n",
        "        if for_regression:\n",
        "            steps.append((\"pt\", PowerTransformer(method=\"yeo-johnson\")))\n",
        "        transformers.append((\"num\", Pipeline(steps), num_idx))\n",
        "    if dcat>0:\n",
        "        oh_kwargs = make_onehot_kwargs()\n",
        "        transformers.append((\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"oh\", OneHotEncoder(**oh_kwargs))\n",
        "        ]), cat_idx))\n",
        "    return ColumnTransformer(transformers)\n",
        "\n",
        "def make_baselines_classif(dnum, dcat):\n",
        "    pre = make_preprocessor(dnum, dcat, for_regression=False)\n",
        "    models = {\n",
        "        \"LogReg\": Pipeline([(\"pre\", pre), (\"clf\", LogisticRegression(max_iter=400))]),\n",
        "        \"RF\": Pipeline([(\"pre\", pre), (\"clf\", RandomForestClassifier(n_estimators=400, min_samples_leaf=2, n_jobs=-1))]),\n",
        "        \"HGB\": Pipeline([(\"pre\", pre), (\"clf\", HistGradientBoostingClassifier(max_depth=None))]),\n",
        "        \"MLP\": Pipeline([(\"pre\", pre), (\"clf\", MLPClassifier(hidden_layer_sizes=(96,48), max_iter=400))]),\n",
        "    }\n",
        "    if HAVE_XGB:\n",
        "        models[\"XGB\"] = Pipeline([(\"pre\", pre), (\"clf\", xgb.XGBClassifier(\n",
        "            n_estimators=600, max_depth=6, subsample=0.9, colsample_bytree=0.9,\n",
        "            tree_method=\"hist\", eval_metric=\"logloss\", n_jobs=-1))])\n",
        "    return models\n",
        "\n",
        "def make_rigel_classif(dnum, dcat):\n",
        "    pre = make_preprocessor(dnum, dcat, for_regression=False)\n",
        "    estimators = [\n",
        "        (\"rf\", Pipeline([(\"pre\", pre), (\"m\", RandomForestClassifier(n_estimators=500, min_samples_leaf=2, n_jobs=-1))])),\n",
        "        (\"hgb\", Pipeline([(\"pre\", pre), (\"m\", HistGradientBoostingClassifier(max_depth=None))])),\n",
        "        (\"log\", Pipeline([(\"pre\", pre), (\"m\", LogisticRegression(max_iter=500))])),\n",
        "        (\"mlp\", Pipeline([(\"pre\", pre), (\"m\", MLPClassifier(hidden_layer_sizes=(96,48), max_iter=500))])),\n",
        "    ]\n",
        "    if HAVE_XGB:\n",
        "        estimators.append((\"xgb\", Pipeline([(\"pre\", pre), (\"m\", xgb.XGBClassifier(\n",
        "            n_estimators=800, max_depth=6, subsample=0.9, colsample_bytree=0.9,\n",
        "            tree_method=\"hist\", eval_metric=\"logloss\", n_jobs=-1))])))\n",
        "    # n_jobs no siempre está en StackingClassifier; evitamos pasarlo para compatibilidad\n",
        "    # Removed preprocessor from final_estimator\n",
        "    stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=600), passthrough=False, cv=5)\n",
        "    rigel = CalibratedClassifierCV(stack, method=\"isotonic\", cv=5)\n",
        "    return rigel\n",
        "\n",
        "def make_baselines_reg(dnum, dcat):\n",
        "    pre = make_preprocessor(dnum, dcat, for_regression=True)\n",
        "    models = {\n",
        "        \"Ridge\": Pipeline([(\"pre\", pre), (\"reg\", Ridge(alpha=1.0))]),\n",
        "        \"RF\": Pipeline([(\"pre\", pre), (\"reg\", RandomForestRegressor(n_estimators=500, min_samples_leaf=2, n_jobs=-1))]),\n",
        "        \"HGB\": Pipeline([(\"pre\", pre), (\"reg\", HistGradientBoostingRegressor(max_depth=None))]),\n",
        "        \"MLP\": Pipeline([(\"pre\", pre), (\"reg\", MLPRegressor(hidden_layer_sizes=(96,48), max_iter=600))]),\n",
        "    }\n",
        "    if HAVE_XGB:\n",
        "        models[\"XGB\"] = Pipeline([(\"pre\", pre), (\"reg\", xgb.XGBRegressor(\n",
        "            n_estimators=800, max_depth=6, subsample=0.9, colsample_bytree=0.9, tree_method=\"hist\", n_jobs=-1))])\n",
        "    return models\n",
        "\n",
        "class RigelRegressor:\n",
        "    \"\"\"\n",
        "    RIGEL++ Regressor:\n",
        "      - Base learners: RF, HGB, Ridge, MLP (+ optional XGB)\n",
        "      - Stacked with Ridge meta-learner (passthrough features)\n",
        "      - Split-conformal intervals for marginal coverage (90% by default)\n",
        "    \"\"\"\n",
        "    def __init__(self, dnum, dcat, alpha=0.10):\n",
        "        self.pre = make_preprocessor(dnum, dcat, for_regression=True)\n",
        "        estimators = [\n",
        "            (\"rf\", Pipeline([(\"pre\", self.pre), (\"m\", RandomForestRegressor(n_estimators=600, min_samples_leaf=2, n_jobs=-1))])),\n",
        "            (\"hgb\", Pipeline([(\"pre\", self.pre), (\"m\", HistGradientBoostingRegressor(max_depth=None))])),\n",
        "            (\"ridge\", Pipeline([(\"pre\", self.pre), (\"m\", Ridge(alpha=1.0))])),\n",
        "            (\"mlp\", Pipeline([(\"pre\", self.pre), (\"m\", MLPRegressor(hidden_layer_sizes=(128,64), max_iter=800))])),\n",
        "        ]\n",
        "        if HAVE_XGB:\n",
        "            estimators.append((\"xgb\", Pipeline([(\"pre\", self.pre), (\"m\", xgb.XGBRegressor(\n",
        "                n_estimators=1000, max_depth=7, subsample=0.9, colsample_bytree=0.9, tree_method=\"hist\", n_jobs=-1))])))\n",
        "        # Removed preprocessor from final_estimator\n",
        "        self.stack = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0), passthrough=False, cv=5)\n",
        "        self.fitted = False\n",
        "        self.q_ = None\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Split-conformal calibration\n",
        "        X_tr, X_cal, y_tr, y_cal = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        self.stack.fit(X_tr, y_tr)\n",
        "        y_cal_pred = self.stack.predict(X_cal)\n",
        "        resid = np.abs(y_cal - y_cal_pred)\n",
        "        q = np.quantile(resid, 1-self.alpha)\n",
        "        self.q_ = float(q)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.stack.predict(X)\n",
        "\n",
        "    def predict_interval(self, X):\n",
        "        assert self.fitted and self.q_ is not None\n",
        "        yhat = self.stack.predict(X)\n",
        "        return yhat - self.q_, yhat + self.q_\n",
        "\n",
        "# ---------------------------- Metrics & Plots ---------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ece_score(y_true, prob, n_bins=15):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    idx = np.digitize(prob, bins) - 1\n",
        "    ece, total = 0.0, len(y_true)\n",
        "    for b in range(n_bins):\n",
        "        mask = idx == b\n",
        "        if not np.any(mask): continue\n",
        "        conf = prob[mask].mean()\n",
        "        acc = (y_true[mask] == (prob[mask]>=0.5)).mean()\n",
        "        ece += (mask.mean()) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "def plot_bar(keys, values, title, ylabel, filename):\n",
        "    plt.figure()\n",
        "    plt.bar(keys, values)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, filename))\n",
        "    plt.close()\n",
        "\n",
        "# ------------------------------- Experiments ----------------------------------\n",
        "def run_classification_suite(n=4000):\n",
        "    specs = [\n",
        "        (\"linear\", dict(kind=\"linear\")),\n",
        "        (\"step\", dict(kind=\"step\")),\n",
        "        (\"sinusoidal\", dict(kind=\"sinusoidal\")),\n",
        "        (\"xor\", dict(kind=\"xor\")),\n",
        "        (\"mixed\", dict(kind=\"mixed\")),\n",
        "    ]\n",
        "    rows = []\n",
        "    for tag, kwargs in specs:\n",
        "        hdr(f\"[Classification] Dataset = {tag}\")\n",
        "        X, y, dnum, dcat = gen_classification_dataset(n=n, d_num=8, d_cat=4, seed=42, **kwargs)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
        "        baselines = make_baselines_classif(dnum, dcat)\n",
        "        rigel = make_rigel_classif(dnum, dcat)\n",
        "        models = {**baselines, \"RIGEL++\": rigel}\n",
        "        for name, model in models.items():\n",
        "            t0 = time.time()\n",
        "            model.fit(Xtr, ytr)\n",
        "            proba = model.predict_proba(Xte)[:,1]\n",
        "            pred = (proba>=0.5).astype(int)\n",
        "            auc = roc_auc_score(yte, proba)\n",
        "            acc = accuracy_score(yte, pred)\n",
        "            ll = log_loss(yte, np.c_[1-proba, proba])\n",
        "            brier = brier_score_loss(yte, proba)\n",
        "            ece = ece_score(yte, proba)\n",
        "            dt = time.time()-t0\n",
        "            rows.append({\"Task\":\"classification\",\"Dataset\":tag,\"Model\":name,\"ROC_AUC\":auc,\"Accuracy\":acc,\"LogLoss\":ll,\"Brier\":brier,\"ECE\":ece,\"TrainTimeSec\":dt})\n",
        "            print(f\"{name:10s} | AUC={auc:.3f} Acc={acc:.3f} LL={ll:.3f} Brier={brier:.3f} ECE={ece:.3f} Time={dt:.1f}s\")\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(OUT_DIR, \"classification_results.csv\"), index=False)\n",
        "    overall = df.groupby(\"Model\", as_index=False).agg(ROC_AUC=(\"ROC_AUC\",\"mean\"), Accuracy=(\"Accuracy\",\"mean\"), ECE=(\"ECE\",\"mean\"))\n",
        "    plot_bar(overall[\"Model\"], overall[\"ROC_AUC\"], \"Classification — Mean ROC AUC\", \"Mean AUC\", \"cls_mean_auc.png\")\n",
        "    plot_bar(overall[\"Model\"], overall[\"ECE\"], \"Classification — Mean ECE\", \"Mean ECE\", \"cls_mean_ece.png\")\n",
        "    return df\n",
        "\n",
        "def run_regression_suite(n=4000):\n",
        "    specs = [\n",
        "        (\"linear\", dict(kind=\"linear\")),\n",
        "        (\"step\", dict(kind=\"step\")),\n",
        "        (\"sinusoidal\", dict(kind=\"sinusoidal\")),\n",
        "        (\"hetero\", dict(kind=\"hetero\")),\n",
        "        (\"mixed\", dict(kind=\"mixed\")),\n",
        "    ]\n",
        "    rows = []\n",
        "    for tag, kwargs in specs:\n",
        "        hdr(f\"[Regression] Dataset = {tag}\")\n",
        "        X, y, dnum, dcat = gen_regression_dataset(n=n, d_num=8, d_cat=4, seed=42, **kwargs)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "        baselines = make_baselines_reg(dnum, dcat)\n",
        "        rigel = RigelRegressor(dnum, dcat, alpha=0.10)\n",
        "        models = {**baselines, \"RIGEL++\": rigel}\n",
        "        for name, model in models.items():\n",
        "            t0=time.time()\n",
        "            if name==\"RIGEL++\":\n",
        "                model.fit(Xtr, ytr)\n",
        "                yhat = model.predict(Xte)\n",
        "                lo, hi = model.predict_interval(Xte)\n",
        "                coverage = np.mean((yte>=lo) & (yte<=hi))\n",
        "                width = np.mean(hi-lo)\n",
        "            else:\n",
        "                model.fit(Xtr, ytr)\n",
        "                yhat = model.predict(Xte)\n",
        "                coverage = np.nan\n",
        "                width = np.nan\n",
        "            rmse = math.sqrt(mean_squared_error(yte, yhat))\n",
        "            mae = mean_absolute_error(yte, yhat)\n",
        "            r2 = r2_score(yte, yhat)\n",
        "            dt=time.time()-t0\n",
        "            rows.append({\"Task\":\"regression\",\"Dataset\":tag,\"Model\":name,\"RMSE\":rmse,\"MAE\":mae,\"R2\":r2,\"PI90_Coverage\":coverage,\"PI90_Width\":width,\"TrainTimeSec\":dt})\n",
        "            print(f\"{name:10s} | R2={r2:.3f} RMSE={rmse:.3f} MAE={mae:.3f} PI90_cov={coverage:.3f} width={width:.3f} Time={dt:.1f}s\")\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(OUT_DIR, \"regression_results.csv\"), index=False)\n",
        "    overall = df.groupby(\"Model\", as_index=False).agg(R2=(\"R2\",\"mean\"))\n",
        "    plot_bar(overall[\"Model\"], overall[\"R2\"], \"Regression — Mean R^2\", \"Mean R^2\", \"reg_mean_r2.png\")\n",
        "    return df\n",
        "\n",
        "def robustness_checks_cls(df_base: pd.DataFrame, n=2000):\n",
        "    hdr(\"[Robustness] Classification stress test\")\n",
        "    X, y, dnum, dcat = gen_classification_dataset(n=n, d_num=8, d_cat=4, seed=24, kind=\"mixed\")\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=9, stratify=y)\n",
        "    Xte_stress = inject_missing_and_outliers(Xte, missing_frac=0.10, outlier_frac=0.05, outlier_scale=15.0)\n",
        "\n",
        "    baselines = make_baselines_classif(dnum, dcat)\n",
        "    rigel = make_rigel_classif(dnum, dcat)\n",
        "    models = {**baselines, \"RIGEL++\": rigel}\n",
        "    rows=[]\n",
        "    for name, model in models.items():\n",
        "        model.fit(Xtr, ytr)\n",
        "        proba = model.predict_proba(Xte_stress)[:,1]\n",
        "        auc = roc_auc_score(yte, proba)\n",
        "        rows.append({\"Model\":name, \"ROC_AUC_Stress\":auc})\n",
        "        print(f\"{name:10s} | Stress AUC={auc:.3f}\")\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(OUT_DIR, \"classification_stress.csv\"), index=False)\n",
        "    return df\n",
        "\n",
        "def robustness_checks_reg(df_base: pd.DataFrame, n=2000):\n",
        "    hdr(\"[Robustness] Regression stress test\")\n",
        "    X, y, dnum, dcat = gen_regression_dataset(n=n, d_num=8, d_cat=4, seed=24, kind=\"hetero\")\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=9)\n",
        "    Xte_stress = inject_missing_and_outliers(Xte, missing_frac=0.10, outlier_frac=0.05, outlier_scale=15.0)\n",
        "\n",
        "    baselines = make_baselines_reg(dnum, dcat)\n",
        "    rigel = RigelRegressor(dnum, dcat, alpha=0.10)\n",
        "    models = {**baselines, \"RIGEL++\": rigel}\n",
        "    rows=[]\n",
        "    for name, model in models.items():\n",
        "        if name==\"RIGEL++\":\n",
        "            model.fit(Xtr, ytr)\n",
        "            yhat = model.predict(Xte_stress)\n",
        "            lo, hi = model.predict_interval(Xte_stress)\n",
        "            coverage = np.mean((yte>=lo) & (yte<=hi))\n",
        "        else:\n",
        "            model.fit(Xtr, ytr)\n",
        "            yhat = model.predict(Xte_stress)\n",
        "            coverage = np.nan\n",
        "        rmse = math.sqrt(mean_squared_error(yte, yhat))\n",
        "        rows.append({\"Model\":name, \"RMSE_Stress\":rmse, \"PI90_Cov_Stress\":coverage})\n",
        "        print(f\"{name:10s} | Stress RMSE={rmse:.3f} Cov={coverage}\")\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(os.path.join(OUT_DIR, \"regression_stress.csv\"), index=False)\n",
        "    return df\n",
        "\n",
        "# ---------------------- Analytical Demonstrations ---------------------\n",
        "ANALYTIC_NOTES = r\"\"\"\n",
        "# Analytical Notes (RIGEL++)\n",
        "\n",
        "**1) Stacking reduces expected generalization error (squared loss).**\n",
        "Let base predictors \\( f_1,\\ldots,f_K \\) and meta-learner \\( g(w) = \\sum_k w_k f_k \\) with \\( w \\) fitted by ridge on OOF predictions. Since squared loss is convex and ridge yields the \\(L_2\\)-projection of \\(y\\) onto the span of \\( \\{f_k\\} \\), we have\n",
        "\\(\\mathbb{E}[(y-g(w))^2] \\le \\min_k \\mathbb{E}[(y-f_k)^2]\\) whenever \\(f_k\\) lie in the hypothesis space and ridge includes the one-hot vectors as feasible \\(w\\) (proof by Pythagorean theorem in Hilbert space with penalty ensuring stability). Equality only if the best base is already in the span and orthogonal to others.\n",
        "\n",
        "**2) Calibration by isotonic regression is risk-monotone.**\n",
        "Given score \\(s\\) and labels \\(y\\), isotonic calibration finds a non-decreasing map \\(h\\) minimizing empirical log-loss among monotone transforms. It cannot worsen the reliability diagram’s monotonicity and typically reduces Brier/log-loss unless already perfectly calibrated.\n",
        "\n",
        "**3) Split-conformal intervals guarantee marginal coverage.**\n",
        "Under exchangeability of \\((X_i,y_i)\\), the split-conformal radius \\(q = \\text{Quantile}_{1-\\alpha}(|y_i-\\hat{y}_i|)\\) computed on a calibration set satisfies\n",
        "\\(\\mathbb{P}\\{ y_{n+1} \\in [\\hat{y}(x_{n+1})-q,\\ \\hat{y}(x_{n+1})+q] \\} \\ge 1-\\alpha\\).\n",
        "This holds for any underlying algorithm \\(\\hat{y}\\); see Vovk et al. (2005).\n",
        "\n",
        "**4) Robust preprocessing (quantile + Yeo–Johnson)** reduces sensitivity to outliers and skew, improving both trees and linear/meta learners, while one-hot on imputed categories keeps variance bounded for sparse high-cardinality features.\n",
        "\"\"\"\n",
        "with open(os.path.join(OUT_DIR, \"ANALYTIC_NOTES.md\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(ANALYTIC_NOTES)\n",
        "\n",
        "# ----------------------------- Validation Checks ------------------------------\n",
        "def run_validations(cls_df: pd.DataFrame, reg_df: pd.DataFrame, cls_stress: pd.DataFrame, reg_stress: pd.DataFrame):\n",
        "    checks = []\n",
        "\n",
        "    # 1) Archivos generados\n",
        "    expected_files = [\n",
        "        \"classification_results.csv\", \"regression_results.csv\",\n",
        "        \"classification_stress.csv\", \"regression_stress.csv\",\n",
        "        \"cls_mean_auc.png\", \"cls_mean_ece.png\", \"reg_mean_r2.png\", \"ANALYTIC_NOTES.md\"\n",
        "    ]\n",
        "    files_present = all(os.path.exists(os.path.join(OUT_DIR, fn)) for fn in expected_files)\n",
        "    checks.append((\"artifacts_exist\", files_present, expected_files))\n",
        "\n",
        "    # 2) RIGEL++ mejora AUC medio vs cada baseline (tolerancia ligera)\n",
        "    rigel_auc = cls_df[cls_df[\"Model\"]==\"RIGEL++\"][\"ROC_AUC\"].mean()\n",
        "    base_auc = cls_df[cls_df[\"Model\"]!=\"RIGEL++\"].groupby(\"Model\")[\"ROC_AUC\"].mean()\n",
        "    rigel_beats_all_auc = bool((rigel_auc >= (base_auc.max() - 1e-6)))\n",
        "    checks.append((\"rigel_auc_ge_best_baseline\", rigel_beats_all_auc, float(rigel_auc), float(base_auc.max())))\n",
        "\n",
        "    # 3) ECE de RIGEL++ <= promedio de ECE baselines\n",
        "    rigel_ece = cls_df[cls_df[\"Model\"]==\"RIGEL++\"][\"ECE\"].mean()\n",
        "    base_ece_mean = cls_df[cls_df[\"Model\"]!=\"RIGEL++\"][\"ECE\"].mean()\n",
        "    ece_ok = bool(rigel_ece <= base_ece_mean + 1e-6)\n",
        "    checks.append((\"rigel_ece_le_baseline_mean\", ece_ok, float(rigel_ece), float(base_ece_mean)))\n",
        "\n",
        "    # 4) Cobertura conformal ~ 0.90 ± 0.05 (promedio)\n",
        "    rigel_cov = reg_df[reg_df[\"Model\"]==\"RIGEL++\"][\"PI90_Coverage\"].mean()\n",
        "    cov_ok = bool((rigel_cov >= 0.85) and (rigel_cov <= 0.95))\n",
        "    checks.append((\"rigel_pi90_coverage_approx_0.90\", cov_ok, float(rigel_cov)))\n",
        "\n",
        "    # 5) RIGEL++ R² medio ≥ mejor baseline R² medio - 0.01\n",
        "    rigel_r2 = reg_df[reg_df[\"Model\"]==\"RIGEL++\"][\"R2\"].mean()\n",
        "    base_r2_max = reg_df[reg_df[\"Model\"]!=\"RIGEL++\"].groupby(\"Model\")[\"R2\"].mean().max()\n",
        "    r2_ok = bool(rigel_r2 >= base_r2_max - 0.01)\n",
        "    checks.append((\"rigel_r2_ge_best_baseline_minus_0.01\", r2_ok, float(rigel_r2), float(base_r2_max)))\n",
        "\n",
        "    # 6) Stress: AUC de RIGEL++ ≥ mediana de baselines en stress\n",
        "    rigel_auc_stress = cls_stress[cls_stress[\"Model\"]==\"RIGEL++\"][\"ROC_AUC_Stress\"].mean()\n",
        "    base_auc_stress_median = cls_stress[cls_stress[\"Model\"]!=\"RIGEL++\"][\"ROC_AUC_Stress\"].median()\n",
        "    stress_auc_ok = bool(rigel_auc_stress >= base_auc_stress_median - 1e-6)\n",
        "    checks.append((\"rigel_stress_auc_ge_baseline_median\", stress_auc_ok, float(rigel_auc_stress), float(base_auc_stress_median)))\n",
        "\n",
        "    # 7) Stress: RMSE de RIGEL++ ≤ mediana de baselines en stress + 0.02\n",
        "    rigel_rmse_stress = reg_stress[reg_stress[\"Model\"]==\"RIGEL++\"][\"RMSE_Stress\"].mean()\n",
        "    base_rmse_stress_median = reg_stress[reg_stress[\"Model\"]!=\"RIGEL++\"][\"RMSE_Stress\"].median()\n",
        "    stress_rmse_ok = bool(rigel_rmse_stress <= base_rmse_stress_median + 0.02)\n",
        "    checks.append((\"rigel_stress_rmse_le_baseline_median_plus_0.02\", stress_rmse_ok, float(rigel_rmse_stress), float(base_rmse_stress_median)))\n",
        "\n",
        "    # 8) XGBoost detectado (solo informativo, no es obligatorio)\n",
        "    checks.append((\"xgboost_available\", HAVE_XGB, None))\n",
        "\n",
        "    # Print scoreboard\n",
        "    hdr(\"SCOREBOARD — Validations (booleans + valores)\")\n",
        "    for item in checks:\n",
        "        name = item[0]\n",
        "        ok = item[1]\n",
        "        vals = item[2:] if len(item)>2 else ()\n",
        "        print(f\"{name:40s}: {1 if ok else 0} | values = {vals}\")\n",
        "    return checks\n",
        "\n",
        "# ----------------------------------- Main ------------------------------------\n",
        "def main():\n",
        "    hdr(\"RIGEL\")\n",
        "    env_report()\n",
        "\n",
        "    cls_df = run_classification_suite(n=3500)\n",
        "    reg_df = run_regression_suite(n=3500)\n",
        "    cls_stress = robustness_checks_cls(cls_df, n=2500)\n",
        "    reg_stress = robustness_checks_reg(reg_df, n=2500)\n",
        "\n",
        "    # Summaries\n",
        "    hdr(\"Summary — Toplines\")\n",
        "    print(\"Classification (mean across datasets):\")\n",
        "    print(cls_df.groupby(\"Model\").agg(ROC_AUC=(\"ROC_AUC\",\"mean\"),\n",
        "                                      Accuracy=(\"Accuracy\",\"mean\"),\n",
        "                                      ECE=(\"ECE\",\"mean\"),\n",
        "                                      TrainTime=(\"TrainTimeSec\",\"mean\")).sort_values(\"ROC_AUC\", ascending=False))\n",
        "\n",
        "    print(\"\\nRegression (mean across datasets):\")\n",
        "    print(reg_df.groupby(\"Model\").agg(R2=(\"R2\",\"mean\"),\n",
        "                                      RMSE=(\"RMSE\",\"mean\"),\n",
        "                                      MAE=(\"MAE\",\"mean\"),\n",
        "                                      TrainTime=(\"TrainTimeSec\",\"mean\")).sort_values(\"R2\", ascending=False))\n",
        "\n",
        "    print(\"\\nRobustness — Classification (Stress AUC):\")\n",
        "    print(cls_stress.sort_values(\"ROC_AUC_Stress\", ascending=False))\n",
        "\n",
        "    print(\"\\nRobustness — Regression (Stress RMSE):\")\n",
        "    print(reg_stress.sort_values(\"RMSE_Stress\"))\n",
        "\n",
        "    checks = run_validations(cls_df, reg_df, cls_stress, reg_stress)\n",
        "\n",
        "    print(f\"\\nArtifacts saved in: {OUT_DIR}\")\n",
        "    print(\"Files:\")\n",
        "    for fn in sorted(os.listdir(OUT_DIR)):\n",
        "        print(\" -\", fn)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpwjr83vzoOy",
        "outputId": "6579c9ac-f42f-46ea-c8b8-2ed2d205046c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "RIGEL++ — Full Benchmark (Colab VM dependent)\n",
            "==========================================================================================\n",
            "Environment: {'python': '3.12.11', 'numpy': '2.0.2', 'pandas': '2.2.2', 'matplotlib': '3.10.0', 'sklearn': '1.6.1'}\n",
            "\n",
            "==========================================================================================\n",
            "[Classification] Dataset = linear\n",
            "==========================================================================================\n",
            "LogReg     | AUC=0.918 Acc=0.842 LL=0.347 Brier=0.109 ECE=0.513 Time=0.3s\n",
            "RF         | AUC=0.900 Acc=0.729 LL=0.524 Brier=0.172 ECE=0.427 Time=2.1s\n",
            "HGB        | AUC=0.893 Acc=0.817 LL=0.400 Brier=0.128 ECE=0.520 Time=1.6s\n",
            "MLP        | AUC=0.878 Acc=0.817 LL=1.300 Brier=0.163 ECE=0.622 Time=6.8s\n",
            "XGB        | AUC=0.888 Acc=0.815 LL=0.617 Brier=0.150 ECE=0.583 Time=3.0s\n",
            "RIGEL++    | AUC=0.920 Acc=0.847 LL=0.344 Brier=0.108 ECE=0.528 Time=254.4s\n",
            "\n",
            "==========================================================================================\n",
            "[Classification] Dataset = step\n",
            "==========================================================================================\n",
            "LogReg     | AUC=0.862 Acc=0.784 LL=0.440 Brier=0.144 ECE=0.440 Time=0.1s\n",
            "RF         | AUC=0.869 Acc=0.728 LL=0.535 Brier=0.177 ECE=0.396 Time=1.6s\n",
            "HGB        | AUC=0.877 Acc=0.801 LL=0.420 Brier=0.136 ECE=0.484 Time=1.8s\n",
            "MLP        | AUC=0.824 Acc=0.761 LL=1.365 Brier=0.211 ECE=0.604 Time=9.4s\n",
            "XGB        | AUC=0.872 Acc=0.798 LL=0.614 Brier=0.162 ECE=0.578 Time=1.3s\n",
            "RIGEL++    | AUC=0.895 Acc=0.822 LL=0.390 Brier=0.127 ECE=0.471 Time=281.5s\n",
            "\n",
            "==========================================================================================\n",
            "[Classification] Dataset = sinusoidal\n",
            "==========================================================================================\n",
            "LogReg     | AUC=0.826 Acc=0.770 LL=0.479 Brier=0.158 ECE=0.426 Time=0.1s\n",
            "RF         | AUC=0.851 Acc=0.717 LL=0.547 Brier=0.182 ECE=0.398 Time=1.6s\n",
            "HGB        | AUC=0.855 Acc=0.789 LL=0.448 Brier=0.145 ECE=0.475 Time=1.4s\n",
            "MLP        | AUC=0.799 Acc=0.752 LL=1.481 Brier=0.219 ECE=0.604 Time=10.2s\n",
            "XGB        | AUC=0.837 Acc=0.777 LL=0.689 Brier=0.175 ECE=0.557 Time=1.3s\n",
            "RIGEL++    | AUC=0.863 Acc=0.802 LL=0.436 Brier=0.141 ECE=0.456 Time=280.0s\n",
            "\n",
            "==========================================================================================\n",
            "[Classification] Dataset = xor\n",
            "==========================================================================================\n",
            "LogReg     | AUC=0.717 Acc=0.689 LL=0.574 Brier=0.198 ECE=0.357 Time=0.5s\n",
            "RF         | AUC=0.755 Acc=0.651 LL=0.597 Brier=0.205 ECE=0.337 Time=2.5s\n",
            "HGB        | AUC=0.893 Acc=0.810 LL=0.394 Brier=0.125 ECE=0.494 Time=1.5s\n",
            "MLP        | AUC=0.806 Acc=0.745 LL=1.521 Brier=0.229 ECE=0.608 Time=8.2s\n",
            "XGB        | AUC=0.893 Acc=0.826 LL=0.564 Brier=0.143 ECE=0.588 Time=3.2s\n",
            "RIGEL++    | AUC=0.899 Acc=0.831 LL=0.382 Brier=0.119 ECE=0.495 Time=284.6s\n",
            "\n",
            "==========================================================================================\n",
            "[Classification] Dataset = mixed\n",
            "==========================================================================================\n",
            "LogReg     | AUC=0.784 Acc=0.730 LL=0.528 Brier=0.176 ECE=0.405 Time=0.2s\n",
            "RF         | AUC=0.816 Acc=0.684 LL=0.566 Brier=0.191 ECE=0.371 Time=2.5s\n",
            "HGB        | AUC=0.808 Acc=0.754 LL=0.513 Brier=0.169 ECE=0.460 Time=1.4s\n",
            "MLP        | AUC=0.771 Acc=0.727 LL=1.740 Brier=0.244 ECE=0.602 Time=11.3s\n",
            "XGB        | AUC=0.794 Acc=0.742 LL=0.816 Brier=0.204 ECE=0.552 Time=1.5s\n",
            "RIGEL++    | AUC=0.848 Acc=0.778 LL=0.460 Brier=0.151 ECE=0.442 Time=291.2s\n",
            "\n",
            "==========================================================================================\n",
            "[Regression] Dataset = linear\n",
            "==========================================================================================\n",
            "Ridge      | R2=0.871 RMSE=0.992 MAE=0.735 PI90_cov=nan width=nan Time=0.3s\n",
            "RF         | R2=0.668 RMSE=1.592 MAE=1.269 PI90_cov=nan width=nan Time=13.1s\n",
            "HGB        | R2=0.816 RMSE=1.184 MAE=0.915 PI90_cov=nan width=nan Time=1.5s\n",
            "MLP        | R2=0.746 RMSE=1.394 MAE=1.087 PI90_cov=nan width=nan Time=15.3s\n",
            "XGB        | R2=0.763 RMSE=1.345 MAE=1.057 PI90_cov=nan width=nan Time=2.1s\n",
            "RIGEL++    | R2=0.886 RMSE=0.935 MAE=0.700 PI90_cov=0.894 width=3.033 Time=125.2s\n",
            "\n",
            "==========================================================================================\n",
            "[Regression] Dataset = step\n",
            "==========================================================================================\n",
            "Ridge      | R2=0.578 RMSE=1.725 MAE=1.416 PI90_cov=nan width=nan Time=0.2s\n",
            "RF         | R2=0.767 RMSE=1.282 MAE=0.951 PI90_cov=nan width=nan Time=13.9s\n",
            "HGB        | R2=0.796 RMSE=1.200 MAE=0.869 PI90_cov=nan width=nan Time=1.5s\n",
            "MLP        | R2=0.620 RMSE=1.636 MAE=1.217 PI90_cov=nan width=nan Time=14.2s\n",
            "XGB        | R2=0.759 RMSE=1.303 MAE=0.962 PI90_cov=nan width=nan Time=2.2s\n",
            "RIGEL++    | R2=0.796 RMSE=1.200 MAE=0.875 PI90_cov=0.890 width=3.802 Time=132.0s\n",
            "\n",
            "==========================================================================================\n",
            "[Regression] Dataset = sinusoidal\n",
            "==========================================================================================\n",
            "Ridge      | R2=0.536 RMSE=1.600 MAE=1.288 PI90_cov=nan width=nan Time=0.2s\n",
            "RF         | R2=0.756 RMSE=1.161 MAE=0.908 PI90_cov=nan width=nan Time=14.0s\n",
            "HGB        | R2=0.845 RMSE=0.927 MAE=0.705 PI90_cov=nan width=nan Time=1.7s\n",
            "MLP        | R2=0.682 RMSE=1.326 MAE=1.017 PI90_cov=nan width=nan Time=15.5s\n",
            "XGB        | R2=0.808 RMSE=1.029 MAE=0.793 PI90_cov=nan width=nan Time=4.0s\n",
            "RIGEL++    | R2=0.832 RMSE=0.963 MAE=0.724 PI90_cov=0.867 width=2.786 Time=132.3s\n",
            "\n",
            "==========================================================================================\n",
            "[Regression] Dataset = hetero\n",
            "==========================================================================================\n",
            "Ridge      | R2=0.741 RMSE=1.075 MAE=0.836 PI90_cov=nan width=nan Time=0.2s\n",
            "RF         | R2=0.647 RMSE=1.255 MAE=0.987 PI90_cov=nan width=nan Time=13.3s\n",
            "HGB        | R2=0.702 RMSE=1.153 MAE=0.901 PI90_cov=nan width=nan Time=1.3s\n",
            "MLP        | R2=0.492 RMSE=1.506 MAE=1.186 PI90_cov=nan width=nan Time=12.0s\n",
            "XGB        | R2=0.652 RMSE=1.247 MAE=0.975 PI90_cov=nan width=nan Time=2.0s\n",
            "RIGEL++    | R2=0.732 RMSE=1.093 MAE=0.852 PI90_cov=0.895 width=3.469 Time=115.7s\n",
            "\n",
            "==========================================================================================\n",
            "[Regression] Dataset = mixed\n",
            "==========================================================================================\n",
            "Ridge      | R2=0.543 RMSE=1.290 MAE=0.938 PI90_cov=nan width=nan Time=0.2s\n",
            "RF         | R2=0.667 RMSE=1.100 MAE=0.823 PI90_cov=nan width=nan Time=14.8s\n",
            "HGB        | R2=0.770 RMSE=0.915 MAE=0.668 PI90_cov=nan width=nan Time=1.7s\n",
            "MLP        | R2=0.595 RMSE=1.213 MAE=0.901 PI90_cov=nan width=nan Time=13.4s\n",
            "XGB        | R2=0.734 RMSE=0.983 MAE=0.714 PI90_cov=nan width=nan Time=2.2s\n",
            "RIGEL++    | R2=0.757 RMSE=0.941 MAE=0.685 PI90_cov=0.908 width=2.855 Time=128.0s\n",
            "\n",
            "==========================================================================================\n",
            "[Robustness] Classification stress test\n",
            "==========================================================================================\n",
            "LogReg     | Stress AUC=0.839\n",
            "RF         | Stress AUC=0.847\n",
            "HGB        | Stress AUC=0.835\n",
            "MLP        | Stress AUC=0.796\n",
            "XGB        | Stress AUC=0.828\n",
            "RIGEL++    | Stress AUC=0.861\n",
            "\n",
            "==========================================================================================\n",
            "[Robustness] Regression stress test\n",
            "==========================================================================================\n",
            "Ridge      | Stress RMSE=2.424 Cov=nan\n",
            "RF         | Stress RMSE=2.524 Cov=nan\n",
            "HGB        | Stress RMSE=2.512 Cov=nan\n",
            "MLP        | Stress RMSE=3.072 Cov=nan\n",
            "XGB        | Stress RMSE=2.631 Cov=nan\n",
            "RIGEL++    | Stress RMSE=2.320 Cov=0.7306666666666667\n",
            "\n",
            "==========================================================================================\n",
            "Summary — Toplines\n",
            "==========================================================================================\n",
            "Classification (mean across datasets):\n",
            "          ROC_AUC  Accuracy       ECE   TrainTime\n",
            "Model                                            \n",
            "RIGEL++  0.884900  0.816000  0.478339  278.366076\n",
            "HGB      0.865208  0.794095  0.486556    1.551571\n",
            "XGB      0.856966  0.791619  0.571564    2.083466\n",
            "RF       0.838135  0.701714  0.385915    2.040607\n",
            "LogReg   0.821361  0.762857  0.428206    0.262258\n",
            "MLP      0.815396  0.760381  0.608011    9.186648\n",
            "\n",
            "Regression (mean across datasets):\n",
            "               R2      RMSE       MAE   TrainTime\n",
            "Model                                            \n",
            "RIGEL++  0.800466  1.026362  0.767204  126.656761\n",
            "HGB      0.785688  1.075872  0.811917    1.526313\n",
            "XGB      0.743321  1.181404  0.899977    2.494633\n",
            "RF       0.700993  1.278218  0.987583   13.804372\n",
            "Ridge    0.653777  1.336530  1.042468    0.200195\n",
            "MLP      0.626990  1.414965  1.081490   14.091736\n",
            "\n",
            "Robustness — Classification (Stress AUC):\n",
            "     Model  ROC_AUC_Stress\n",
            "5  RIGEL++        0.860840\n",
            "1       RF        0.847313\n",
            "0   LogReg        0.838881\n",
            "2      HGB        0.834697\n",
            "4      XGB        0.827994\n",
            "3      MLP        0.795536\n",
            "\n",
            "Robustness — Regression (Stress RMSE):\n",
            "     Model  RMSE_Stress  PI90_Cov_Stress\n",
            "5  RIGEL++     2.320324         0.730667\n",
            "0    Ridge     2.423970              NaN\n",
            "2      HGB     2.512153              NaN\n",
            "1       RF     2.524363              NaN\n",
            "4      XGB     2.631225              NaN\n",
            "3      MLP     3.071666              NaN\n",
            "\n",
            "==========================================================================================\n",
            "SCOREBOARD — Validations (booleans + valores)\n",
            "==========================================================================================\n",
            "artifacts_exist                         : 1 | values = (['classification_results.csv', 'regression_results.csv', 'classification_stress.csv', 'regression_stress.csv', 'cls_mean_auc.png', 'cls_mean_ece.png', 'reg_mean_r2.png', 'ANALYTIC_NOTES.md'],)\n",
            "rigel_auc_ge_best_baseline              : 1 | values = (0.8848997513706488, 0.8652078286370012)\n",
            "rigel_ece_le_baseline_mean              : 1 | values = (0.4783385906911877, 0.49605051125486577)\n",
            "rigel_pi90_coverage_approx_0.90         : 1 | values = (0.8908571428571428,)\n",
            "rigel_r2_ge_best_baseline_minus_0.01    : 1 | values = (0.8004656825347072, 0.7856878993056515)\n",
            "rigel_stress_auc_ge_baseline_median     : 1 | values = (0.8608395382305092, 0.8346968464522588)\n",
            "rigel_stress_rmse_le_baseline_median_plus_0.02: 1 | values = (2.320323699625712, 2.5243628680417234)\n",
            "xgboost_available                       : 1 | values = (None,)\n",
            "\n",
            "Artifacts saved in: /content/rigel_out\n",
            "Files:\n",
            " - ANALYTIC_NOTES.md\n",
            " - classification_results.csv\n",
            " - classification_stress.csv\n",
            " - cls_mean_auc.png\n",
            " - cls_mean_ece.png\n",
            " - reg_mean_r2.png\n",
            " - regression_results.csv\n",
            " - regression_stress.csv\n"
          ]
        }
      ]
    }
  ]
}